{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import csv\n",
    "from collections import namedtuple, Counter, defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path('ThirtyMusic/')\n",
    "entities_dir = data_root / 'entities'\n",
    "relations_dir = data_root / 'relations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_path = entities_dir / 'tracks.idomaar'\n",
    "\n",
    "with tracks_path.open() as tracks_file:\n",
    "    tracks_reader = csv.reader(tracks_file, delimiter='\\t')\n",
    "    track_authors = {\n",
    "        int(track_id): json.loads(linked_entities)['artists'][0]['id']\n",
    "        for _, track_id, _, _, linked_entities in tracks_reader\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В статье, описывающей датасет указано, что прослушивания, продолжительность которых не достигает половины трека или 4 минут по правилам last.fm не должны попадать в историю воспроизведения. Будем рассматривать короткие воспроизведения как ошибки скробблинга или аномальные файлы. Прослушивания короче 30 секунд будем отбрасывать. При других методах сбора данных короткие воспроизведения можно было бы рассматривать как отрицательный сигнал, считая, что они происходят, когда пользователь проматывает трек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ArtistsSession = namedtuple('ArtistsSesson', 'id user timestamp artists')\n",
    "\n",
    "MIN_PLAYTIME = 30\n",
    "sessions_path = relations_dir / 'sessions.idomaar'\n",
    "\n",
    "\n",
    "raw_artist_sessions = []\n",
    "artist_sessions = []\n",
    "with sessions_path.open() as sessions_file:\n",
    "    for session_line in sessions_file:\n",
    "        _, session_id, timestamp, _, linked_entities = session_line.split()\n",
    "        linked_entities_dict = json.loads(linked_entities)\n",
    "        \n",
    "        listened_track_ids = []\n",
    "        all_listened_artist_ids = []\n",
    "        listened_artist_ids = []\n",
    "        prev_track_author = None\n",
    "        user_id = linked_entities_dict['subjects'][0]['id']\n",
    "        for track in linked_entities_dict['objects']:\n",
    "            if track['playtime'] < MIN_PLAYTIME:\n",
    "                continue\n",
    "            track_id = track['id']\n",
    "            \n",
    "            track_author_id = track_authors[track_id]\n",
    "            all_listened_artist_ids.append(track_author_id)\n",
    "            \n",
    "            if prev_track_author != track_author_id:\n",
    "                listened_artist_ids.append(track_author_id)\n",
    "            prev_track_author = track_author_id\n",
    "        \n",
    "        raw_artist_sessions.append(ArtistsSession(int(session_id), user_id, int(timestamp), all_listened_artist_ids))\n",
    "        artist_sessions.append(ArtistsSession(int(session_id), user_id, int(timestamp), listened_artist_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_path = entities_dir / 'persons.idomaar'\n",
    "\n",
    "with artists_path.open() as artists_file:\n",
    "    artists_reader = csv.reader(artists_file, delimiter='\\t')\n",
    "    artist_names = {\n",
    "        int(artist_id): json.loads(artist_properties)['name']\n",
    "        for _, artist_id, _, artist_properties, _ in artists_reader\n",
    "    }\n",
    "artist_ids = {artist_name: artist_id for artist_id, artist_name in artist_names.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Среди исполнителей много дупликатов из-за непрвильно прописанных тегов. Не будем рассматривать исполнителей с менее чем 100 прослушиваниями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_ARTIST_PLAYS = 100\n",
    "\n",
    "artist_play_counter = Counter(artist for session in artist_sessions for artist in session.artists)\n",
    "popular_artists = {\n",
    "    artist_id \n",
    "    for artist_id, n_listens in artist_play_counter.most_common() \n",
    "    if n_listens >= MIN_ARTIST_PLAYS\n",
    "}\n",
    "artists_number = len(popular_artists)\n",
    "artists_list = list(popular_artists)\n",
    "artists_indexes = {artist_id: i for i, artist_id in enumerate(artists_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nontrivial_sessions = []\n",
    "for session in artist_sessions:\n",
    "    artists = [artist for artist in session.artists if artist in popular_artists]\n",
    "    if len(artists) > 1:\n",
    "        nontrivial_sessions.append(ArtistsSession(session.id, session.user, session.timestamp, artists))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Протокол оценки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобъём на обучающую, валидационную и тестовую выборки по времени, чтобы исключить смешение тренировочных данных с оценочными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_timestamps = np.array([session.timestamp for session in artist_sessions])\n",
    "train_val_threshold = np.percentile(session_timestamps, 60)\n",
    "val_test_threshold = np.percentile(session_timestamps, 80)\n",
    "\n",
    "def split_tvt(sessions):\n",
    "    train_sessions = []\n",
    "    val_sessions = []\n",
    "    test_sessions = []\n",
    "    for session in nontrivial_sessions:\n",
    "        if session.timestamp < train_val_threshold:\n",
    "            train_sessions.append(session)\n",
    "        elif session.timestamp < val_test_threshold:\n",
    "            val_sessions.append(session)\n",
    "        else:\n",
    "            test_sessions.append(session)\n",
    "    return train_sessions, val_sessions, test_sessions\n",
    "\n",
    "nt_train, nt_val, nt_test = split_tvt(nontrivial_sessions)\n",
    "ra_train, ra_val, ra_test = split_tvt(raw_artist_sessions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим только пользователей, попавших во все три части, чтобы избежать проблемы холодного старта."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_users = {session.user for session in nt_train}\n",
    "val_users = {session.user for session in nt_val}\n",
    "test_users = {session.user for session in nt_test}\n",
    "common_users = train_users & val_users & test_users\n",
    "common_users_list = list(common_users)\n",
    "users_indexes = {user_id: i for i, user_id in enumerate(common_users_list)}\n",
    "users_number = len(common_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Релевантных исполнителей будем искать по скалярному произведению векторов. \n",
    "Это не расстояние, но оно лучше соответствуем использованным моделям."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarArtistsFinder:\n",
    "    def __init__(self, embeddings, artists_list, artists_indexes):\n",
    "        self.vectors = embeddings\n",
    "        self.artists_list = artists_list\n",
    "        self.artists_indexes = artists_indexes\n",
    "        \n",
    "    def get_closest_by_id(self, artist_id, k):\n",
    "        artist_vector = self.vectors[self.artists_indexes[artist_id]]\n",
    "        closest_ids = self.get_closest_by_vector(artist_vector, k + 1)\n",
    "        closest_ids = [i for i in closest_ids if i != artist_id][:k]\n",
    "        return closest_ids\n",
    "    \n",
    "    def get_closest_by_vector(self, artist_vector, k):\n",
    "        with torch.no_grad():\n",
    "            artist_vector = artist_vector.to(device)\n",
    "            dists = self.vectors @ artist_vector\n",
    "            _, closest_artist_indexes = torch.topk(dists, k)\n",
    "        closest_artist_ids = [self.artists_list[int(index)] for index in closest_artist_indexes]\n",
    "        return closest_artist_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для рекомендаций будем использовать эмбеддинги пользователей и выбирать похожих на них исполнителей. \n",
    "Другой возможный вариант - пройти по прослушанным исполнителям и взять их соседей, но такой подход вычислительно сложнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(users_embeddings, neighbours_finder, artists_list, users_list):\n",
    "    return {\n",
    "        users_list[user]: neighbours_finder.get_closest_by_vector(user_embedding, 20)\n",
    "        for user, user_embedding in enumerate(users_embeddings)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users_play_counters(sessions):\n",
    "    played_by_users = defaultdict(Counter)\n",
    "    for session in sessions:\n",
    "        played_by_users[session.user].update(session.artists)\n",
    "    return played_by_users\n",
    "\n",
    "users_play_counters_val = get_users_play_counters(nt_val)\n",
    "users_play_counters_test = get_users_play_counters(nt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_k(recommendations, played, k):\n",
    "    users_prs = []\n",
    "    for user, user_recs in recommendations.items():\n",
    "        user_played = played[user]\n",
    "        correct = 0\n",
    "        for rec in user_recs[:k]:\n",
    "            if rec in user_played:\n",
    "                correct += 1\n",
    "        users_prs.append(correct / k)\n",
    "    return np.mean(users_prs)\n",
    "\n",
    "\n",
    "def mean_average_precision(recommendations, played, k):\n",
    "    users_aps = []\n",
    "    for user, user_recs in recommendations.items():\n",
    "        user_played = played[user]\n",
    "        correct = 0\n",
    "        precs = []\n",
    "        for i, rec in enumerate(user_recs[:k], 1):\n",
    "            if rec in user_played:\n",
    "                correct += 1\n",
    "            precs.append(correct / i)\n",
    "        users_aps.append(precs)\n",
    "    return np.mean(users_aps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве показателя релевантности для ndcg возьмём долю прослушиваний данного исполнителя. \n",
    "Количество воспроизведений используется вместо доли для упрощения вычислений, поскольку при нормализации \n",
    "сумма по исполнителям сокращается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNTS = 1 / np.log(np.arange(2, 102))\n",
    "\n",
    "\n",
    "def ndcg(recommendations, play_counters, k):\n",
    "    users_ndcgs = []\n",
    "    for user, user_recs in recommendations.items():\n",
    "        user_counter = play_counters[user]\n",
    "        if user_counter:\n",
    "            gains = np.array([user_counter[rec_art] for rec_art in user_recs[:k]])\n",
    "            tgt_gains = np.array([count for _, count in user_counter.most_common()[:k]])\n",
    "            c_k = min(k, len(user_counter))\n",
    "            users_ndcgs.append(np.sum(gains[:c_k] * DISCOUNTS[:c_k]) / np.sum(tgt_gains[:c_k] * DISCOUNTS[:c_k]))\n",
    "    return np.mean(users_ndcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SAMPLE_ARTISTS = 228054, 287560, 4807, 315200, 310487\n",
    "\n",
    "def show_neighbours(embeddings):\n",
    "    neighbours_finder = SimilarArtistsFinder(next(embeddings.parameters()), artists_list, artists_indexes)\n",
    "    for artist_id in SAMPLE_ARTISTS:\n",
    "        print(artist_names[artist_id])\n",
    "        for i in neighbours_finder.get_closest_by_id(artist_id, 20):\n",
    "            print(artist_names[i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, n_epochs, lr, device='cpu', verbose=True):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_state = None\n",
    "    for epoch in range(n_epochs):\n",
    "        train_losses = []\n",
    "        for batch in train_data:\n",
    "            opt.zero_grad()\n",
    "            loss = model.calc_loss(batch, device)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            train_losses.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_losses = [model.calc_loss(batch, device).cpu().numpy() for batch in val_data]\n",
    "\n",
    "        train_loss, val_loss = np.mean(train_losses), np.mean(val_losses)\n",
    "        if verbose:\n",
    "            print(f'Epoch {epoch + 1} train_loss: {train_loss:.3f} val_loss: {val_loss:.3f}')\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_state = model.state_dict()\n",
    "    model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель на основе SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextDataset(IterableDataset):\n",
    "    def __init__(self, sessions, max_context_dist):\n",
    "        self.sessions_artists = [\n",
    "            [artists_indexes[artist_id] for artist_id in session.artists] \n",
    "            for session in sessions\n",
    "        ]\n",
    "        self.max_dist = max_context_dist\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for session_artists in self.sessions_artists:\n",
    "            for target_index, target in enumerate(session_artists):\n",
    "                for neighbour in session_artists[max(0, target_index - self.max_dist):target_index + self.max_dist + 1]:\n",
    "                    if neighbour != target:\n",
    "                        yield neighbour, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_SGL = 2 ** 19\n",
    "CONTEXT_DIST = 4\n",
    "\n",
    "sgl_train_dl = DataLoader(ContextDataset(nt_train, CONTEXT_DIST), batch_size=BATCH_SIZE_SGL)\n",
    "sgl_val_dl = DataLoader(ContextDataset(nt_val, CONTEXT_DIST), batch_size=BATCH_SIZE_SGL)\n",
    "sgl_test_dl = DataLoader(ContextDataset(nt_test, CONTEXT_DIST), batch_size=BATCH_SIZE_SGL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SAMPLE_OPTIONS_NUMBER = 100000000\n",
    "artist_sample_probs = np.array([artist_play_counter[i] for i in artists_list]) ** .75\n",
    "artist_sample_probs /= artist_sample_probs.sum()\n",
    "artist_samples = torch.tensor(list(chain.from_iterable(\n",
    "    [i] * int(TARGET_SAMPLE_OPTIONS_NUMBER * p) for i, p in enumerate(artist_sample_probs)\n",
    ")))\n",
    "SAMPLE_OPTIONS_NUMBER = len(artist_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель, использующая архитектуру skip-gram для обучения представлений слов.\n",
    "Сессии прослушивания рассматриваются как тексты, а исполнители - как слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramLikeModel(nn.Module):\n",
    "    def __init__(self, vocab_size, dimensionality):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeddings = nn.Embedding(vocab_size, dimensionality)\n",
    "        self.context_embeddings = nn.Embedding(vocab_size, dimensionality)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embeddings(x)\n",
    "    \n",
    "    def sample_negatives(self, sample_size):\n",
    "        return artist_samples[torch.randint(0, SAMPLE_OPTIONS_NUMBER, (sample_size,), dtype=torch.long)]\n",
    "    \n",
    "    def calc_loss(self, batch, device):\n",
    "        batch_samples, batch_target = batch\n",
    "        batch_samples, batch_target = batch_samples.to(device), batch_target.to(device)\n",
    "        \n",
    "        out = self.context_embeddings(batch_samples)\n",
    "        target = self.embeddings(batch_target)\n",
    "        neg_samples = self.context_embeddings(self.sample_negatives(len(batch_target)).to(device))\n",
    "                \n",
    "        out_log_sigm = F.logsigmoid(-torch.sum(target * out, dim=1))\n",
    "        neg_log_sigm = F.logsigmoid(torch.sum(target * neg_samples, dim=1))\n",
    "        return neg_log_sigm.mean() + out_log_sigm.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgl_model = SkipGramLikeModel(artists_number, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: -8224.831 val_loss: -36698.660\n",
      "Epoch 2 train_loss: -152277.766 val_loss: -323900.188\n",
      "Epoch 3 train_loss: -627397.375 val_loss: -986848.438\n",
      "Epoch 4 train_loss: -1491180.500 val_loss: -2034366.250\n",
      "Epoch 5 train_loss: -2735778.750 val_loss: -3449782.250\n"
     ]
    }
   ],
   "source": [
    "train(sgl_model, sgl_train_dl, sgl_val_dl, 5, .5, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для получения вектора пользователя векторы прослушанных исполнителей усредняются с весами, пропорциональными колиеству прослушиваний. \n",
    "Рассматривалось также скользящее среднее, но такое усреднение оказалось слишком сложным вычислительно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_art_counters_train = defaultdict(Counter)\n",
    "for session in ra_train:\n",
    "    if session.user in common_users:\n",
    "        users_art_counters_train[session.user].update(session.artists)\n",
    "\n",
    "user_art_shares_train = {}\n",
    "for user, user_counter in users_art_counters_train.items():\n",
    "    c_sum = sum(user_counter.values())\n",
    "    listened_artists, artist_shares = [], []\n",
    "    for art, c in user_counter.most_common():\n",
    "        listened_artists.append(artists_indexes[art])\n",
    "        artist_shares.append(c / c_sum)\n",
    "    user_art_shares_train[user] = torch.LongTensor(listened_artists), torch.tensor(artist_shares)\n",
    "\n",
    "def make_users_embedding(artists_embedding, users_indexes, artists_indexes):\n",
    "    with torch.no_grad():\n",
    "        users_embedding = torch.zeros((users_number, artists_embedding.shape[1]), dtype=torch.float32)\n",
    "        for user_index, user_id in enumerate(common_users_list):\n",
    "            arts, shares = user_art_shares_train[user_id]\n",
    "            users_embedding[user_index] = torch.sum(artists_embedding[arts] * shares.reshape(-1, 1), dim=0)\n",
    "    return users_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель на основе факторизации матриц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorDataset(Dataset):\n",
    "    def __init__(self, artists_sessions, user_indexes, artist_indexes):\n",
    "        user_artist_counters = Counter(\n",
    "            (session.user, artist_id)\n",
    "            for session in artists_sessions \n",
    "            for artist_id in session.artists\n",
    "        )\n",
    "        self.data = [\n",
    "            (user_indexes[user_id], artist_indexes[artist_id], c) \n",
    "            for (user_id, artist_id), c in user_artist_counters.items()\n",
    "            if user_id in user_indexes and artist_id in artist_indexes\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_MF = 2 ** 18\n",
    "\n",
    "mf_train_dl = DataLoader(\n",
    "    MatrixFactorDataset(ra_train, users_indexes, artists_indexes), batch_size=BATCH_SIZE_MF, shuffle=True)\n",
    "mf_val_dl = DataLoader(\n",
    "    MatrixFactorDataset(ra_val, users_indexes, artists_indexes), batch_size=BATCH_SIZE_MF)\n",
    "mf_test_dl = DataLoader(\n",
    "    MatrixFactorDataset(ra_test, users_indexes, artists_indexes), batch_size=BATCH_SIZE_MF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта модель раскладывает матрицу прослушиваний, в ячейке которой стоит едниница, если соответствующий пользователь слушал треки соответствующего исполнителя и ноль в противном случае, на матрицы из векторных представлений пользователей и исполнителей. Вес ячейки в функции потерь увеличивается с ростом числа прослушиваний. Оценки также могли бы влиять на функцию ошибки, но в имеющихся данных они не имеют временных меток, что мешает правильно разделить их на тренировочные и валидационные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorizationModel(nn.Module):\n",
    "    def __init__(self, num_artists, num_users, dimensionality, reg_param, alpha, beta):\n",
    "        super().__init__()\n",
    "        self.user_embeddings = nn.Embedding(num_users, dimensionality)\n",
    "        self.artist_embeddings = nn.Embedding(num_artists, dimensionality)\n",
    "        self.num_artists = num_artists\n",
    "        self.num_users = num_users\n",
    "        self.reg_param = reg_param\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, users, artists):\n",
    "        return torch.sum(self.user_embeddings(users) * self.artist_embeddings(artists), dim=1)\n",
    "\n",
    "    def calc_loss(self, batch, device):\n",
    "        users, artists, counts = batch\n",
    "        users, artists, counts = users.to(device), artists.to(device), counts.to(device)\n",
    "        weights = 1 + self.alpha * torch.log((1 + counts * self.beta).float())\n",
    "        neg_users = torch.randint_like(users, self.num_users)\n",
    "        neg_artists = torch.randint_like(artists, self.num_artists)\n",
    "        return torch.mean(self.forward(neg_users, neg_artists) ** 2) + \\\n",
    "                    torch.mean((1 - self.forward(users, artists)) ** 2 * weights) + \\\n",
    "                    self.reg_param * (torch.norm(self.user_embeddings(users), dim=1).mean() + \n",
    "                                      torch.norm(self.artist_embeddings(artists), dim=1).mean() +\n",
    "                                      torch.norm(self.user_embeddings(neg_users), dim=1).mean() + \n",
    "                                      torch.norm(self.artist_embeddings(neg_artists), dim=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf_model = MatrixFactorizationModel(artists_number, users_number, 20, .1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 30.165 val_loss: 10.060\n",
      "Epoch 2 train_loss: 6.015 val_loss: 4.277\n",
      "Epoch 3 train_loss: 3.383 val_loss: 3.038\n",
      "Epoch 4 train_loss: 2.347 val_loss: 2.050\n",
      "Epoch 5 train_loss: 1.452 val_loss: 1.419\n",
      "Epoch 6 train_loss: 1.099 val_loss: 1.190\n",
      "Epoch 7 train_loss: 0.970 val_loss: 1.085\n",
      "Epoch 8 train_loss: 0.902 val_loss: 1.018\n",
      "Epoch 9 train_loss: 0.856 val_loss: 0.972\n",
      "Epoch 10 train_loss: 0.821 val_loss: 0.937\n",
      "Epoch 11 train_loss: 0.794 val_loss: 0.907\n",
      "Epoch 12 train_loss: 0.771 val_loss: 0.883\n",
      "Epoch 13 train_loss: 0.751 val_loss: 0.862\n",
      "Epoch 14 train_loss: 0.733 val_loss: 0.844\n",
      "Epoch 15 train_loss: 0.717 val_loss: 0.829\n",
      "Epoch 16 train_loss: 0.704 val_loss: 0.816\n",
      "Epoch 17 train_loss: 0.692 val_loss: 0.804\n",
      "Epoch 18 train_loss: 0.683 val_loss: 0.795\n",
      "Epoch 19 train_loss: 0.675 val_loss: 0.787\n",
      "Epoch 20 train_loss: 0.668 val_loss: 0.781\n",
      "Epoch 21 train_loss: 0.662 val_loss: 0.776\n",
      "Epoch 22 train_loss: 0.657 val_loss: 0.771\n",
      "Epoch 23 train_loss: 0.653 val_loss: 0.767\n",
      "Epoch 24 train_loss: 0.650 val_loss: 0.764\n",
      "Epoch 25 train_loss: 0.647 val_loss: 0.761\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7609735"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(mf_model, mf_train_dl, mf_val_dl, 25, 0.05, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оптимизация гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 5/5 [33:43<00:00, 404.67s/trial, best loss: -0.10982363734250662]\n",
      "50 4\n"
     ]
    }
   ],
   "source": [
    "def sgl_model_obj(config):\n",
    "    train_data = DataLoader(ContextDataset(nt_train, config['context']), batch_size=BATCH_SIZE_SGL)\n",
    "    val_data = DataLoader(ContextDataset(nt_val, config['context']), batch_size=BATCH_SIZE_SGL)\n",
    "    \n",
    "    model = SkipGramLikeModel(ARTISTS_NUMBER, config['dim'])\n",
    "    train(model, train_data, val_data, 5, 0.5, device, False)\n",
    "    \n",
    "    artists_embeddings = next(iter(model.embeddings.parameters()))\n",
    "    users_embeddings = make_users_embedding(artists_embeddings.cpu(), users_indexes, artists_indexes)\n",
    "    artists_embeddings, users_embeddings = artists_embeddings.to(device), users_embeddings.to(device)\n",
    "    \n",
    "    neighbour_finder = SimilarArtistsFinder(artists_embeddings, artists_list, artists_indexes)\n",
    "    recs = recommend(users_embeddings, neighbour_finder, artists_list, common_users_list)\n",
    "    return -mean_average_precision(recs, users_play_counters_val, 20)\n",
    "\n",
    "\n",
    "sgl_dim_options = [20, 50, 100]\n",
    "sgl_context_options = [2, 4]\n",
    "sgl_space = {\n",
    "    'dim': hp.choice('dim', sgl_dim_options), \n",
    "    'context': hp.choice('context', sgl_context_options) \n",
    "}\n",
    "\n",
    "best_sgl_params = fmin(sgl_model_obj, sgl_space, algo=tpe.suggest, max_evals=5)\n",
    "\n",
    "sgl_dim = sgl_dim_options[best_sgl_params['dim']]\n",
    "sgl_context = sgl_context_options[best_sgl_params['context']]\n",
    "print(sgl_dim, sgl_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 40/40 [1:12:41<00:00, 109.03s/trial, best loss: -0.12017143732796823]\n",
      "20 0.1 1.0 5\n"
     ]
    }
   ],
   "source": [
    "def mf_obj(config):\n",
    "    model = MatrixFactorizationModel(artists_number, users_number, \n",
    "                                     config['dim'], config['reg'], config['alpha'], config['beta'])\n",
    "    train(model, mf_train_dl, mf_val_dl, 20, 0.1, device, False)\n",
    "    \n",
    "    artist_embeddings = next(iter(model.artist_embeddings.parameters())).detach()\n",
    "    user_embeddings = next(iter(model.user_embeddings.parameters())).detach()\n",
    "    \n",
    "    neighbour_finder = SimilarArtistsFinder(artist_embeddings, artists_list, artists_indexes)\n",
    "    recs = recommend(user_embeddings, neighbour_finder, artists_list, common_users_list)\n",
    "    return -mean_average_precision(recs, users_play_counters_val, 20)\n",
    "\n",
    "\n",
    "mf_dim_options = [10, 20, 50]\n",
    "mf_reg_options = [.01, .1, 1.]\n",
    "mf_alpha_options = [1., 5., 20.]\n",
    "mf_beta_options = [1, 5, 20]\n",
    "mf_space = {\n",
    "    'dim': hp.choice('dim', mf_dim_options),\n",
    "    'reg': hp.choice('reg', mf_reg_options),\n",
    "    'alpha': hp.choice('alpha', mf_alpha_options),\n",
    "    'beta': hp.choice('beta', mf_beta_options)\n",
    "}\n",
    "\n",
    "best_mf_params = fmin(mf_obj, mf_space, algo=tpe.suggest, max_evals=40)\n",
    "mf_dim = mf_dim_options[best_mf_params['dim']]\n",
    "mf_reg = mf_reg_options[best_mf_params['reg']]\n",
    "mf_alpha = mf_alpha_options[best_mf_params['alpha']]\n",
    "mf_beta = mf_beta_options[best_mf_params['beta']]\n",
    "print(mf_dim, mf_reg, mf_alpha, mf_beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оценка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: -8635.944 val_loss: -40078.363\n",
      "Epoch 2 train_loss: -174113.859 val_loss: -372984.750\n",
      "Epoch 3 train_loss: -706580.938 val_loss: -1100334.750\n",
      "Epoch 4 train_loss: -1630167.500 val_loss: -2205818.250\n",
      "Epoch 5 train_loss: -2925555.000 val_loss: -3667587.250\n"
     ]
    }
   ],
   "source": [
    "sgl_opt_model = SkipGramLikeModel(ARTISTS_NUMBER, sgl_dim)\n",
    "sgl_train_dl = DataLoader(ContextDataset(nt_train, sgl_context), batch_size=BATCH_SIZE_SGL)\n",
    "sgl_val_dl = DataLoader(ContextDataset(nt_val, sgl_context), batch_size=BATCH_SIZE_SGL)\n",
    "train(sgl_opt_model, sgl_train_dl, sgl_val_dl, 5, 0.5, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 28.171 val_loss: 6.163\n",
      "Epoch 2 train_loss: 4.323 val_loss: 3.127\n",
      "Epoch 3 train_loss: 1.755 val_loss: 1.403\n",
      "Epoch 4 train_loss: 1.048 val_loss: 1.157\n",
      "Epoch 5 train_loss: 0.913 val_loss: 1.066\n",
      "Epoch 6 train_loss: 0.861 val_loss: 1.017\n",
      "Epoch 7 train_loss: 0.826 val_loss: 0.980\n",
      "Epoch 8 train_loss: 0.799 val_loss: 0.951\n",
      "Epoch 9 train_loss: 0.777 val_loss: 0.930\n",
      "Epoch 10 train_loss: 0.758 val_loss: 0.913\n",
      "Epoch 11 train_loss: 0.744 val_loss: 0.901\n",
      "Epoch 12 train_loss: 0.733 val_loss: 0.891\n",
      "Epoch 13 train_loss: 0.725 val_loss: 0.884\n",
      "Epoch 14 train_loss: 0.718 val_loss: 0.878\n",
      "Epoch 15 train_loss: 0.714 val_loss: 0.874\n",
      "Epoch 16 train_loss: 0.711 val_loss: 0.871\n",
      "Epoch 17 train_loss: 0.709 val_loss: 0.870\n",
      "Epoch 18 train_loss: 0.707 val_loss: 0.867\n",
      "Epoch 19 train_loss: 0.706 val_loss: 0.867\n",
      "Epoch 20 train_loss: 0.705 val_loss: 0.866\n"
     ]
    }
   ],
   "source": [
    "mf_opt_model = MatrixFactorizationModel(artists_number, users_number, mf_dim, mf_reg, mf_alpha, mf_beta)\n",
    "train(mf_opt_model, mf_train_dl, mf_val_dl, 20, 0.1, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgl_artists_embeddings = next(iter(sgl_opt_model.embeddings.parameters()))\n",
    "sgl_users_embeddings = make_users_embedding(sgl_artists_embeddings.cpu(), users_indexes, artists_indexes)\n",
    "sgl_artists_embeddings, sgl_users_embeddings = sgl_artists_embeddings.to(device), sgl_users_embeddings.to(device)\n",
    "sgl_neighbour_finder = SimilarArtistsFinder(sgl_artists_embeddings, artists_list, artists_indexes)\n",
    "sgl_recs = recommend(sgl_users_embeddings, sgl_neighbour_finder, artists_list, common_users_list)\n",
    "\n",
    "mf_artist_embeddings = next(iter(mf_opt_model.artist_embeddings.parameters()))\n",
    "mf_user_embeddings = next(iter(mf_opt_model.user_embeddings.parameters()))\n",
    "mf_neighbour_finder = SimilarArtistsFinder(mf_artist_embeddings, artists_list, artists_indexes)\n",
    "mf_recs = recommend(mf_user_embeddings, mf_neighbour_finder, artists_list, common_users_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkipGram metrics: pr1 0.078 pr10 0.074 map 0.074 ndcg 0.041\n",
      "MatrixFactorization metrics: pr1 0.089 pr10 0.099 map 0.098 ndcg 0.065\n"
     ]
    }
   ],
   "source": [
    "sgl_pr1 = precision_k(sgl_recs, users_play_counters_test, 1)\n",
    "sgl_pr10 = precision_k(sgl_recs, users_play_counters_test, 10)\n",
    "sgl_map = mean_average_precision(sgl_recs, users_play_counters_test, 20)\n",
    "sgl_ndcg = ndcg(sgl_recs, users_play_counters_test, 20)\n",
    "\n",
    "mf_pr1 = precision_k(mf_recs, users_play_counters_test, 1)\n",
    "mf_pr10 = precision_k(mf_recs, users_play_counters_test, 10)\n",
    "mf_map = mean_average_precision(mf_recs, users_play_counters_test, 20)\n",
    "mf_ndcg = ndcg(mf_recs, users_play_counters_test, 20)\n",
    "\n",
    "print(f'SkipGram metrics: pr1 {sgl_pr1:.3f} pr10 {sgl_pr10:.3f} map {sgl_map:.3f} ndcg {sgl_ndcg:.3f}')\n",
    "print(f'MatrixFactorization metrics: pr1 {mf_pr1:.3f} pr10 {mf_pr10:.3f} map {mf_map:.3f} ndcg {mf_ndcg:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metallica+&+Korn\n",
      "Guns+Nroses\n",
      "Cure,+The\n",
      "Johnny+Cash+&+Waylon+Jennings\n",
      "RED+HOT+CHILI+PAPPERS\n",
      "The+Beatles,+Brian+Matthew\n",
      "Simon\n",
      "Foo+Fighter\n",
      "Nirvana\n",
      "Bruce+Springsteen++&+The+E+Street+Band\n",
      "Nine+Inch+Nails\n",
      "OASIS+-+T.R.W.M.\n",
      "P.J.+Harvey\n",
      "The+Pet+Shop+Boys\n",
      "Santana+feat.+Dave+Matthews+&+Carter+Beauford\n",
      "Tool\n",
      "Black+Sabbath\n",
      "Jamiroquai\n",
      "Sonic+Youth+&+Cypress+Hill\n",
      "Neil+Young+&+Graham+Nash+with+The+Stray+Gators\n",
      "Lenny+Krawitz\n",
      "\n",
      "Rammstein\n",
      "Cure,+The\n",
      "Johnny+Cash+&+Waylon+Jennings\n",
      "Simon\n",
      "The+Beatles,+Brian+Matthew\n",
      "RED+HOT+CHILI+PAPPERS\n",
      "Foo+Fighter\n",
      "Guns+Nroses\n",
      "Nine+Inch+Nails\n",
      "Nirvana\n",
      "P.J.+Harvey\n",
      "The+Pet+Shop+Boys\n",
      "OASIS+-+T.R.W.M.\n",
      "Bruce+Springsteen++&+The+E+Street+Band\n",
      "Jamiroquai\n",
      "Sonic+Youth+&+Cypress+Hill\n",
      "Santana+feat.+Dave+Matthews+&+Carter+Beauford\n",
      "Tool\n",
      "Lenny+Krawitz\n",
      "Alanis+Morissette\n",
      "Neil+Young+&+Graham+Nash+with+The+Stray+Gators\n",
      "\n",
      "50+Cent+ft+Lloyd+Banks+&+Tony+Yayo\n",
      "Ellie+Goulding\n",
      "Justin+Timberlake\n",
      "Sia+Furler\n",
      "Lana+Del+Rey+(www.hitov.ru)\n",
      "John+Legend+&+Teyana+Taylor\n",
      "Calvin+Harris+Ft.+John+Newman\n",
      "Ed+Sheeran+&+Gary+Lightbody\n",
      "Drake+Feat+Bun+B+&+Lil+Wayne\n",
      "Sam+Smith\n",
      "Michael+Jackson%3B\n",
      "Adele\n",
      "Maroon+5+&+Christina+Aguilera\n",
      "Kanye+West+&+DJ+Khaled\n",
      "Lorde\n",
      "Disclosure+feat.+London+Grammar\n",
      "Icona+Pop\n",
      "Bastille+&+MNEK\n",
      "frank+ocean+feat.+John+Mayer\n",
      "Beyonce\n",
      "P!nk+&+William+Orbit\n",
      "\n",
      "Shakira\n",
      "Ellie+Goulding\n",
      "Justin+Timberlake\n",
      "Sia+Furler\n",
      "Lana+Del+Rey+(www.hitov.ru)\n",
      "John+Legend+&+Teyana+Taylor\n",
      "Ed+Sheeran+&+Gary+Lightbody\n",
      "Calvin+Harris+Ft.+John+Newman\n",
      "Sam+Smith\n",
      "Michael+Jackson%3B\n",
      "Drake+Feat+Bun+B+&+Lil+Wayne\n",
      "Maroon+5+&+Christina+Aguilera\n",
      "Adele\n",
      "Lorde\n",
      "Kanye+West+&+DJ+Khaled\n",
      "Disclosure+feat.+London+Grammar\n",
      "Icona+Pop\n",
      "frank+ocean+feat.+John+Mayer\n",
      "Bastille+&+MNEK\n",
      "Macklemore+&+Ryan+Lewis+%5Bfeat.+Evan+Roman%5D\n",
      "Beyonce\n",
      "\n",
      "Scorpions\n",
      "Guns+Nroses\n",
      "RED+HOT+CHILI+PAPPERS\n",
      "Foo+Fighter\n",
      "Johnny+Cash+&+Waylon+Jennings\n",
      "Cure,+The\n",
      "The+Beatles,+Brian+Matthew\n",
      "Nirvana\n",
      "Simon\n",
      "Black+Sabbath\n",
      "Bruce+Springsteen++&+The+E+Street+Band\n",
      "Nine+Inch+Nails\n",
      "OASIS+-+T.R.W.M.\n",
      "Santana+feat.+Dave+Matthews+&+Carter+Beauford\n",
      "Rammstein\n",
      "Tool\n",
      "Lenny+Krawitz\n",
      "The+Pet+Shop+Boys\n",
      "Jamiroquai\n",
      "P.J.+Harvey\n",
      "Lynyrd+Skynyrd\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_neighbours(sgl_opt_model.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metallica+&+Korn\n",
      "%23\n",
      "RED+HOT+CHILI+PAPPERS\n",
      "Nirvana\n",
      "Pearl+Jam+&+Zeke\n",
      "Pink+Floud\n",
      "Muse\n",
      "Daft+Punk+&+M83+VS+Big+Black+Delta\n",
      "Led+Zeppelin\n",
      "The+Beatles,+Brian+Matthew\n",
      "Foo+Fighter\n",
      "Queen+&+Wyclef+Jean+(featuring+Pras+Michael+&+Free)\n",
      "Alice+In+Chains+with+Pearl+Jam\n",
      "System+of+a+Down\n",
      "Arctic+Monkeys\n",
      "Queens+Of+The+Stone+Age+&+Bea\n",
      "U2+&+Sin%C3%A9ad+O%E2%80%99Connor\n",
      "Nine+Inch+Nails\n",
      "Radiohead+&+Sigur+R%C3%B3s\n",
      "Linkin+Park+&+Jay+Gordon+of+Orgy\n",
      "Placebo+feat.+Alison+Mosshart\n",
      "\n",
      "Rammstein\n",
      "%23\n",
      "System+of+a+Down\n",
      "Muse\n",
      "Marilyn+Manson+&+Rasputina\n",
      "RED+HOT+CHILI+PAPPERS\n",
      "Linkin+Park+&+Jay+Gordon+of+Orgy\n",
      "Nine+Inch+Nails\n",
      "Nirvana\n",
      "Depeche+Mode\n",
      "Daft+Punk+&+M83+VS+Big+Black+Delta\n",
      "Placebo+feat.+Alison+Mosshart\n",
      "Queen+&+Wyclef+Jean+(featuring+Pras+Michael+&+Free)\n",
      "Pink+Floud\n",
      "Metallica+&+Korn\n",
      "Pearl+Jam+&+Zeke\n",
      "The+Prodigy\n",
      "Foo+Fighter\n",
      "Alice+In+Chains+with+Pearl+Jam\n",
      "U2+&+Sin%C3%A9ad+O%E2%80%99Connor\n",
      "Led+Zeppelin\n",
      "\n",
      "50+Cent+ft+Lloyd+Banks+&+Tony+Yayo\n",
      "%23\n",
      "Eminem+Featuring+Dido\n",
      "Daft+Punk+&+M83+VS+Big+Black+Delta\n",
      "Pharell+wiliams\n",
      "Lana+Del+Rey+(www.hitov.ru)\n",
      "Kanye+West+&+DJ+Khaled\n",
      "Michael+Jackson%3B\n",
      "Kanye+West,+JAY+Z+&+Big+Sean\n",
      "Justin+Timberlake\n",
      "Coldplay\n",
      "Rihanna+feat.+Drake\n",
      "OutKast\n",
      "Katy+Perry+-+www.SongsLover.com\n",
      "Arctic+Monkeys\n",
      "Amy+Whinehouse\n",
      "Iggy+Azalea+feat.+T.I.\n",
      "RED+HOT+CHILI+PAPPERS\n",
      "Adele\n",
      "Lorde\n",
      "Beyonce\n",
      "\n",
      "Shakira\n",
      "%23\n",
      "Lana+Del+Rey+(www.hitov.ru)\n",
      "Daft+Punk+&+M83+VS+Big+Black+Delta\n",
      "Coldplay\n",
      "Lady+Gaga+Featuring+Colby+O%27Donis\n",
      "Katy+Perry+-+www.SongsLover.com\n",
      "Michael+Jackson%3B\n",
      "Arctic+Monkeys\n",
      "Pharell+wiliams\n",
      "Madonna&Justin+Timberlake\n",
      "Rihanna+feat.+Drake\n",
      "Adele\n",
      "Lorde\n",
      "Imagine+Dragons+&+Daft+Punk\n",
      "Amy+Whinehouse\n",
      "Lilly+Allen\n",
      "Eminem+Featuring+Dido\n",
      "Beyonce\n",
      "Sia+Furler\n",
      "Taylor+Swift+feat.+The+Civil+Wars\n",
      "\n",
      "Scorpions\n",
      "%23\n",
      "Queen+&+Wyclef+Jean+(featuring+Pras+Michael+&+Free)\n",
      "RED+HOT+CHILI+PAPPERS\n",
      "Aerosmith+&+Carrie+Underwood\n",
      "The+Beatles,+Brian+Matthew\n",
      "Metallica+&+Korn\n",
      "Pink+Floud\n",
      "Guns+Nroses\n",
      "Led+Zeppelin\n",
      "AC+DC\n",
      "Alice+In+Chains+with+Pearl+Jam\n",
      "Pearl+Jam+&+Zeke\n",
      "System+of+a+Down\n",
      "U2+&+Sin%C3%A9ad+O%E2%80%99Connor\n",
      "Muse\n",
      "Linkin+Park+&+Jay+Gordon+of+Orgy\n",
      "Foo+Fighter\n",
      "Nirvana\n",
      "Black+Sabbath\n",
      "Marilyn+Manson+&+Rasputina\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_neighbours(mf_opt_model.artist_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Статистическая значимость"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy.stats import mannwhitneyu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SUBSAMPLES = 5\n",
    "SUBSAMPLE_SHARE = .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбиение на тренировочную и валидационную выборки по времени не позволяет использовать обычную k-fold кросс-валидацию, поэтому будем на каждом шаге брать случайные подмножества тренировочной и валидационной выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07111244467781862 0.04591553521866327 0.048022814098212185 0.036295484825726324\n",
      "0.0672867751856575 0.052464181231715555 0.053813431628133616 0.04059481935565944\n",
      "0.055809766709174105 0.052989273122796485 0.052892195053751284 0.039524833620521775\n",
      "0.06068561998349711 0.04223989198109669 0.043478994660156034 0.033479416459550194\n",
      "0.05130897907133748 0.03732653214312505 0.03711909884758768 0.0289581776970256\n"
     ]
    }
   ],
   "source": [
    "sgl_pr1s, sgl_pr10s, sgl_maps, sgl_ndcgs = [], [], [], []\n",
    "\n",
    "for _ in range(N_SUBSAMPLES):\n",
    "    tr_sub = random.sample(nt_train + nt_val, int(SUBSAMPLE_SHARE * (len(nt_train) + len(nt_val))))\n",
    "    te_sub = random.sample(nt_test, int(SUBSAMPLE_SHARE * len(nt_test)))\n",
    "    train_data = DataLoader(ContextDataset(tr_sub, sgl_context), batch_size=BATCH_SIZE_SGL)\n",
    "    val_data = DataLoader(ContextDataset(te_sub, sgl_context), batch_size=BATCH_SIZE_SGL)\n",
    "    \n",
    "    model = SkipGramLikeModel(ARTISTS_NUMBER, sgl_dim)\n",
    "    train(model, train_data, val_data, int(5 / SUBSAMPLE_SHARE), 0.5, device, False)\n",
    "    \n",
    "    artists_embeddings = next(iter(model.embeddings.parameters()))\n",
    "    users_embeddings = make_users_embedding(artists_embeddings.cpu(), users_indexes, artists_indexes)\n",
    "    artists_embeddings, users_embeddings = artists_embeddings.to(device), users_embeddings.to(device)\n",
    "    \n",
    "    neighbour_finder = SimilarArtistsFinder(artists_embeddings, artists_list, artists_indexes)\n",
    "    recs = recommend(users_embeddings, neighbour_finder, artists_list, common_users_list)\n",
    "    te_ua_counters = get_users_play_counters(te_sub)\n",
    "    \n",
    "    c_pr1 = precision_k(recs, te_ua_counters, 1)\n",
    "    c_pr10 = precision_k(recs, te_ua_counters, 10)\n",
    "    c_map = mean_average_precision(recs, te_ua_counters, 20)\n",
    "    c_ndcg = ndcg(recs, te_ua_counters, 20)\n",
    "    print(c_pr1, c_pr10, c_map, c_ndcg)\n",
    "    sgl_pr1s.append(c_pr1)\n",
    "    sgl_pr10s.append(c_pr10)\n",
    "    sgl_maps.append(c_map)\n",
    "    sgl_ndcgs.append(c_ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train_loss: 31.889 val_loss: 7.403\n",
      "Epoch 2 train_loss: 4.944 val_loss: 3.798\n",
      "Epoch 3 train_loss: 2.453 val_loss: 1.775\n",
      "Epoch 4 train_loss: 1.235 val_loss: 1.241\n",
      "Epoch 5 train_loss: 0.989 val_loss: 1.100\n",
      "Epoch 6 train_loss: 0.906 val_loss: 1.026\n",
      "Epoch 7 train_loss: 0.863 val_loss: 0.985\n",
      "Epoch 8 train_loss: 0.832 val_loss: 0.950\n",
      "Epoch 9 train_loss: 0.806 val_loss: 0.924\n",
      "Epoch 10 train_loss: 0.784 val_loss: 0.903\n",
      "Epoch 11 train_loss: 0.765 val_loss: 0.886\n",
      "Epoch 12 train_loss: 0.751 val_loss: 0.871\n",
      "Epoch 13 train_loss: 0.739 val_loss: 0.863\n",
      "Epoch 14 train_loss: 0.730 val_loss: 0.853\n",
      "Epoch 15 train_loss: 0.723 val_loss: 0.846\n",
      "Epoch 16 train_loss: 0.717 val_loss: 0.841\n",
      "Epoch 17 train_loss: 0.712 val_loss: 0.839\n",
      "Epoch 18 train_loss: 0.709 val_loss: 0.835\n",
      "Epoch 19 train_loss: 0.706 val_loss: 0.832\n",
      "Epoch 20 train_loss: 0.704 val_loss: 0.830\n",
      "Epoch 21 train_loss: 0.702 val_loss: 0.829\n",
      "Epoch 22 train_loss: 0.701 val_loss: 0.828\n",
      "Epoch 23 train_loss: 0.700 val_loss: 0.826\n",
      "Epoch 24 train_loss: 0.699 val_loss: 0.825\n",
      "Epoch 25 train_loss: 0.698 val_loss: 0.825\n",
      "Epoch 26 train_loss: 0.698 val_loss: 0.825\n",
      "Epoch 27 train_loss: 0.698 val_loss: 0.825\n",
      "Epoch 28 train_loss: 0.697 val_loss: 0.825\n",
      "Epoch 29 train_loss: 0.697 val_loss: 0.825\n",
      "Epoch 30 train_loss: 0.697 val_loss: 0.825\n",
      "Epoch 31 train_loss: 0.697 val_loss: 0.825\n",
      "Epoch 32 train_loss: 0.697 val_loss: 0.825\n",
      "Epoch 33 train_loss: 0.697 val_loss: 0.826\n",
      "Epoch 34 train_loss: 0.698 val_loss: 0.826\n",
      "Epoch 35 train_loss: 0.697 val_loss: 0.826\n",
      "Epoch 36 train_loss: 0.698 val_loss: 0.826\n",
      "Epoch 37 train_loss: 0.699 val_loss: 0.826\n",
      "Epoch 38 train_loss: 0.698 val_loss: 0.827\n",
      "Epoch 39 train_loss: 0.699 val_loss: 0.827\n",
      "Epoch 40 train_loss: 0.700 val_loss: 0.828\n",
      "0.06421123696646913 0.06272597704598305 0.06259229189096603 0.05411398621690822\n",
      "Epoch 1 train_loss: 31.794 val_loss: 7.371\n",
      "Epoch 2 train_loss: 4.974 val_loss: 3.922\n",
      "Epoch 3 train_loss: 2.621 val_loss: 1.881\n",
      "Epoch 4 train_loss: 1.272 val_loss: 1.262\n",
      "Epoch 5 train_loss: 0.993 val_loss: 1.107\n",
      "Epoch 6 train_loss: 0.904 val_loss: 1.031\n",
      "Epoch 7 train_loss: 0.859 val_loss: 0.986\n",
      "Epoch 8 train_loss: 0.828 val_loss: 0.952\n",
      "Epoch 9 train_loss: 0.802 val_loss: 0.926\n",
      "Epoch 10 train_loss: 0.782 val_loss: 0.905\n",
      "Epoch 11 train_loss: 0.764 val_loss: 0.889\n",
      "Epoch 12 train_loss: 0.751 val_loss: 0.876\n",
      "Epoch 13 train_loss: 0.739 val_loss: 0.867\n",
      "Epoch 14 train_loss: 0.730 val_loss: 0.858\n",
      "Epoch 15 train_loss: 0.723 val_loss: 0.851\n",
      "Epoch 16 train_loss: 0.717 val_loss: 0.846\n",
      "Epoch 17 train_loss: 0.713 val_loss: 0.841\n",
      "Epoch 18 train_loss: 0.709 val_loss: 0.839\n",
      "Epoch 19 train_loss: 0.707 val_loss: 0.836\n",
      "Epoch 20 train_loss: 0.704 val_loss: 0.834\n",
      "Epoch 21 train_loss: 0.703 val_loss: 0.831\n",
      "Epoch 22 train_loss: 0.701 val_loss: 0.830\n",
      "Epoch 23 train_loss: 0.700 val_loss: 0.831\n",
      "Epoch 24 train_loss: 0.699 val_loss: 0.830\n",
      "Epoch 25 train_loss: 0.699 val_loss: 0.829\n",
      "Epoch 26 train_loss: 0.698 val_loss: 0.827\n",
      "Epoch 27 train_loss: 0.698 val_loss: 0.827\n",
      "Epoch 28 train_loss: 0.697 val_loss: 0.828\n",
      "Epoch 29 train_loss: 0.697 val_loss: 0.827\n",
      "Epoch 30 train_loss: 0.697 val_loss: 0.827\n",
      "Epoch 31 train_loss: 0.697 val_loss: 0.828\n",
      "Epoch 32 train_loss: 0.697 val_loss: 0.827\n",
      "Epoch 33 train_loss: 0.697 val_loss: 0.827\n",
      "Epoch 34 train_loss: 0.697 val_loss: 0.827\n",
      "Epoch 35 train_loss: 0.698 val_loss: 0.827\n",
      "Epoch 36 train_loss: 0.698 val_loss: 0.829\n",
      "Epoch 37 train_loss: 0.699 val_loss: 0.828\n",
      "Epoch 38 train_loss: 0.699 val_loss: 0.829\n",
      "Epoch 39 train_loss: 0.699 val_loss: 0.828\n",
      "Epoch 40 train_loss: 0.699 val_loss: 0.829\n",
      "0.05265921536268847 0.06026554647063237 0.05896665358469772 0.05096910110336061\n",
      "Epoch 1 train_loss: 31.675 val_loss: 7.360\n",
      "Epoch 2 train_loss: 4.974 val_loss: 3.937\n",
      "Epoch 3 train_loss: 2.667 val_loss: 1.882\n",
      "Epoch 4 train_loss: 1.273 val_loss: 1.259\n",
      "Epoch 5 train_loss: 0.995 val_loss: 1.105\n",
      "Epoch 6 train_loss: 0.905 val_loss: 1.029\n",
      "Epoch 7 train_loss: 0.861 val_loss: 0.986\n",
      "Epoch 8 train_loss: 0.831 val_loss: 0.951\n",
      "Epoch 9 train_loss: 0.805 val_loss: 0.924\n",
      "Epoch 10 train_loss: 0.784 val_loss: 0.902\n",
      "Epoch 11 train_loss: 0.765 val_loss: 0.886\n",
      "Epoch 12 train_loss: 0.750 val_loss: 0.872\n",
      "Epoch 13 train_loss: 0.739 val_loss: 0.861\n",
      "Epoch 14 train_loss: 0.730 val_loss: 0.853\n",
      "Epoch 15 train_loss: 0.723 val_loss: 0.848\n",
      "Epoch 16 train_loss: 0.717 val_loss: 0.842\n",
      "Epoch 17 train_loss: 0.712 val_loss: 0.839\n",
      "Epoch 18 train_loss: 0.709 val_loss: 0.836\n",
      "Epoch 19 train_loss: 0.707 val_loss: 0.833\n",
      "Epoch 20 train_loss: 0.704 val_loss: 0.831\n",
      "Epoch 21 train_loss: 0.703 val_loss: 0.830\n",
      "Epoch 22 train_loss: 0.701 val_loss: 0.829\n",
      "Epoch 23 train_loss: 0.700 val_loss: 0.829\n",
      "Epoch 24 train_loss: 0.699 val_loss: 0.828\n",
      "Epoch 25 train_loss: 0.698 val_loss: 0.826\n",
      "Epoch 26 train_loss: 0.697 val_loss: 0.826\n",
      "Epoch 27 train_loss: 0.697 val_loss: 0.826\n",
      "Epoch 28 train_loss: 0.697 val_loss: 0.826\n",
      "Epoch 29 train_loss: 0.697 val_loss: 0.826\n",
      "Epoch 30 train_loss: 0.696 val_loss: 0.826\n",
      "Epoch 31 train_loss: 0.697 val_loss: 0.826\n",
      "Epoch 32 train_loss: 0.697 val_loss: 0.827\n",
      "Epoch 33 train_loss: 0.697 val_loss: 0.827\n",
      "Epoch 34 train_loss: 0.697 val_loss: 0.826\n",
      "Epoch 35 train_loss: 0.698 val_loss: 0.827\n",
      "Epoch 36 train_loss: 0.698 val_loss: 0.828\n",
      "Epoch 37 train_loss: 0.698 val_loss: 0.827\n",
      "Epoch 38 train_loss: 0.698 val_loss: 0.829\n",
      "Epoch 39 train_loss: 0.699 val_loss: 0.829\n",
      "Epoch 40 train_loss: 0.699 val_loss: 0.828\n",
      "0.0574600555097142 0.060558097667091734 0.060004851792673126 0.05181446485435935\n",
      "Epoch 1 train_loss: 32.112 val_loss: 7.441\n",
      "Epoch 2 train_loss: 5.055 val_loss: 4.052\n",
      "Epoch 3 train_loss: 2.920 val_loss: 2.096\n",
      "Epoch 4 train_loss: 1.357 val_loss: 1.298\n",
      "Epoch 5 train_loss: 1.009 val_loss: 1.116\n",
      "Epoch 6 train_loss: 0.905 val_loss: 1.034\n",
      "Epoch 7 train_loss: 0.859 val_loss: 0.986\n",
      "Epoch 8 train_loss: 0.827 val_loss: 0.954\n",
      "Epoch 9 train_loss: 0.802 val_loss: 0.926\n",
      "Epoch 10 train_loss: 0.781 val_loss: 0.905\n",
      "Epoch 11 train_loss: 0.763 val_loss: 0.887\n",
      "Epoch 12 train_loss: 0.749 val_loss: 0.874\n",
      "Epoch 13 train_loss: 0.738 val_loss: 0.862\n",
      "Epoch 14 train_loss: 0.728 val_loss: 0.856\n",
      "Epoch 15 train_loss: 0.721 val_loss: 0.848\n",
      "Epoch 16 train_loss: 0.715 val_loss: 0.843\n",
      "Epoch 17 train_loss: 0.710 val_loss: 0.839\n",
      "Epoch 18 train_loss: 0.708 val_loss: 0.837\n",
      "Epoch 19 train_loss: 0.705 val_loss: 0.834\n",
      "Epoch 20 train_loss: 0.702 val_loss: 0.833\n",
      "Epoch 21 train_loss: 0.701 val_loss: 0.830\n",
      "Epoch 22 train_loss: 0.700 val_loss: 0.828\n",
      "Epoch 23 train_loss: 0.699 val_loss: 0.829\n",
      "Epoch 24 train_loss: 0.698 val_loss: 0.828\n",
      "Epoch 25 train_loss: 0.697 val_loss: 0.827\n",
      "Epoch 26 train_loss: 0.696 val_loss: 0.826\n",
      "Epoch 27 train_loss: 0.696 val_loss: 0.826\n",
      "Epoch 28 train_loss: 0.696 val_loss: 0.825\n",
      "Epoch 29 train_loss: 0.696 val_loss: 0.826\n",
      "Epoch 30 train_loss: 0.696 val_loss: 0.825\n",
      "Epoch 31 train_loss: 0.695 val_loss: 0.825\n",
      "Epoch 32 train_loss: 0.696 val_loss: 0.827\n",
      "Epoch 33 train_loss: 0.696 val_loss: 0.826\n",
      "Epoch 34 train_loss: 0.696 val_loss: 0.826\n",
      "Epoch 35 train_loss: 0.697 val_loss: 0.826\n",
      "Epoch 36 train_loss: 0.697 val_loss: 0.826\n",
      "Epoch 37 train_loss: 0.697 val_loss: 0.827\n",
      "Epoch 38 train_loss: 0.697 val_loss: 0.827\n",
      "Epoch 39 train_loss: 0.697 val_loss: 0.827\n",
      "Epoch 40 train_loss: 0.698 val_loss: 0.828\n",
      "0.0630110269297127 0.06391118445728003 0.06360540711820002 0.05425263747880456\n",
      "Epoch 1 train_loss: 31.860 val_loss: 7.385\n",
      "Epoch 2 train_loss: 4.999 val_loss: 4.014\n",
      "Epoch 3 train_loss: 2.821 val_loss: 2.002\n",
      "Epoch 4 train_loss: 1.318 val_loss: 1.278\n",
      "Epoch 5 train_loss: 1.000 val_loss: 1.105\n",
      "Epoch 6 train_loss: 0.903 val_loss: 1.028\n",
      "Epoch 7 train_loss: 0.858 val_loss: 0.981\n",
      "Epoch 8 train_loss: 0.827 val_loss: 0.948\n",
      "Epoch 9 train_loss: 0.802 val_loss: 0.921\n",
      "Epoch 10 train_loss: 0.780 val_loss: 0.900\n",
      "Epoch 11 train_loss: 0.764 val_loss: 0.883\n",
      "Epoch 12 train_loss: 0.749 val_loss: 0.869\n",
      "Epoch 13 train_loss: 0.738 val_loss: 0.860\n",
      "Epoch 14 train_loss: 0.728 val_loss: 0.851\n",
      "Epoch 15 train_loss: 0.722 val_loss: 0.845\n",
      "Epoch 16 train_loss: 0.716 val_loss: 0.839\n",
      "Epoch 17 train_loss: 0.712 val_loss: 0.836\n",
      "Epoch 18 train_loss: 0.708 val_loss: 0.832\n",
      "Epoch 19 train_loss: 0.705 val_loss: 0.829\n",
      "Epoch 20 train_loss: 0.703 val_loss: 0.827\n",
      "Epoch 21 train_loss: 0.701 val_loss: 0.827\n",
      "Epoch 22 train_loss: 0.700 val_loss: 0.824\n",
      "Epoch 23 train_loss: 0.699 val_loss: 0.827\n",
      "Epoch 24 train_loss: 0.699 val_loss: 0.823\n",
      "Epoch 25 train_loss: 0.698 val_loss: 0.823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 train_loss: 0.697 val_loss: 0.823\n",
      "Epoch 27 train_loss: 0.697 val_loss: 0.822\n",
      "Epoch 28 train_loss: 0.697 val_loss: 0.823\n",
      "Epoch 29 train_loss: 0.697 val_loss: 0.822\n",
      "Epoch 30 train_loss: 0.697 val_loss: 0.823\n",
      "Epoch 31 train_loss: 0.697 val_loss: 0.822\n",
      "Epoch 32 train_loss: 0.697 val_loss: 0.823\n",
      "Epoch 33 train_loss: 0.696 val_loss: 0.823\n",
      "Epoch 34 train_loss: 0.697 val_loss: 0.823\n",
      "Epoch 35 train_loss: 0.697 val_loss: 0.823\n",
      "Epoch 36 train_loss: 0.697 val_loss: 0.824\n",
      "Epoch 37 train_loss: 0.698 val_loss: 0.821\n",
      "Epoch 38 train_loss: 0.698 val_loss: 0.825\n",
      "Epoch 39 train_loss: 0.699 val_loss: 0.824\n",
      "Epoch 40 train_loss: 0.699 val_loss: 0.824\n",
      "0.06173580376565899 0.06331107943890181 0.06268200158655272 0.05412528194693056\n"
     ]
    }
   ],
   "source": [
    "mf_pr1s, mf_pr10s, mf_maps, mf_ndcgs = [], [], [], []\n",
    "\n",
    "for _ in range(N_SUBSAMPLES):\n",
    "    tr_sub = random.sample(ra_train + ra_val, int(SUBSAMPLE_SHARE * (len(ra_train) + len(ra_val))))\n",
    "    te_sub = random.sample(ra_test, int(SUBSAMPLE_SHARE * len(ra_test)))\n",
    "    train_data = DataLoader(MatrixFactorDataset(tr_sub, users_indexes, artists_indexes), batch_size=BATCH_SIZE_MF, \n",
    "                            shuffle=True)\n",
    "    val_data = DataLoader(MatrixFactorDataset(te_sub, users_indexes, artists_indexes), batch_size=BATCH_SIZE_MF)\n",
    "    \n",
    "    model = MatrixFactorizationModel(artists_number, users_number, mf_dim, mf_reg, mf_alpha, mf_beta)\n",
    "    train(model, train_data, val_data, int(20 / SUBSAMPLE_SHARE), 0.1, device)\n",
    "    \n",
    "    artist_embeddings = next(iter(model.artist_embeddings.parameters())).detach()\n",
    "    user_embeddings = next(iter(model.user_embeddings.parameters())).detach()\n",
    "    \n",
    "    neighbour_finder = SimilarArtistsFinder(artist_embeddings, artists_list, artists_indexes)\n",
    "    recs = recommend(user_embeddings, neighbour_finder, artists_list, common_users_list)\n",
    "    te_ua_counters = get_users_play_counters(te_sub)\n",
    "    \n",
    "    c_pr1 = precision_k(recs, te_ua_counters, 1)\n",
    "    c_pr10 = precision_k(recs, te_ua_counters, 10)\n",
    "    c_map = mean_average_precision(recs, te_ua_counters, 20)\n",
    "    c_ndcg = ndcg(recs, te_ua_counters, 20)\n",
    "    print(c_pr1, c_pr10, c_map, c_ndcg)\n",
    "    mf_pr1s.append(c_pr1)\n",
    "    mf_pr10s.append(c_pr10)\n",
    "    mf_maps.append(c_map)\n",
    "    mf_ndcgs.append(c_ndcg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5 0.006092890177672406 0.006092890177672406 0.006092890177672406\n"
     ]
    }
   ],
   "source": [
    "_, p_value_pr1 = mannwhitneyu(mf_pr1s, sgl_pr1s)\n",
    "_, p_value_pr10 = mannwhitneyu(mf_pr10s, sgl_pr10s)\n",
    "_, p_value_map = mannwhitneyu(mf_maps, sgl_maps)\n",
    "_, p_value_ndcg = mannwhitneyu(mf_ndcgs, sgl_ndcgs)\n",
    "\n",
    "print(p_value_pr1, p_value_pr10, p_value_map, p_value_ndcg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-value для precision@10, mean average precision и ndcg ниже общепринятого порога в 0.05, что позволяет считать различия в этих метриках значимыми. Метрики отличаются от полученных на полных выборках, что говорит о том, что уменьшение размера данных влияет на качество рекомендаций."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
